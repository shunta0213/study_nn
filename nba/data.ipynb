{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(1991, 2023))\n",
    "url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape MVPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# Already fetched\n",
    "# for year in years:\n",
    "#   url = url_start.format(year)\n",
    "#   data = requests.get(url)\n",
    "#   file_name = \"./mvps/{}.html\".format(year)\n",
    "\n",
    "\n",
    "#   if os.path.exists(file_name):\n",
    "#     os.remove(file_name)\n",
    "#   with open(file_name, \"w+\") as f:\n",
    "#     f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    with open(\"./mvps/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "\n",
    "    soup = bs(page, \"html.parser\")\n",
    "    soup.find(\"tr\", class_=\"over_header\").decompose()\n",
    "    mvp_table = soup.find_all(id=\"mvp\")\n",
    "    mvp = pd.read_html(str(mvp_table))[0]\n",
    "    mvp[\"Year\"] = year\n",
    "\n",
    "    print(year)\n",
    "\n",
    "    dfs.append(mvp)\n",
    "    mvps = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"mvps.csv\"):\n",
    "    os.remove(\"mvps.csv\")\n",
    "mvps.to_csv(\"mvps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape player stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = webdriver.FirefoxOptions()\n",
    "# driver = webdriver.Remote(command_executor=os.environ[\"SELENIUM_URL\"], options=options)\n",
    "\n",
    "# for year in years:\n",
    "#     url = player_stats_url.format(year)\n",
    "#     print(f\"scraping {year} data\")\n",
    "\n",
    "#     driver.get(url)\n",
    "#     driver.execute_script(\"window.scrollTo(1,10000)\")\n",
    "#     time.sleep(2)\n",
    "\n",
    "#     with open(\"player/{}.html\".format(year), \"w+\") as f:\n",
    "#         f.write(driver.page_source)\n",
    "\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for year in years:\n",
    "    with open(\"player/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "\n",
    "    soup = bs(page, \"html.parser\")\n",
    "    soup.find(\"tr\", class_=\"thead\").decompose()\n",
    "    player_table = soup.find_all(id=\"per_game_stats\")[0]\n",
    "    player_df = pd.read_html(str(player_table))[0]\n",
    "    player_df[\"Year\"] = year\n",
    "    dfs.append(player_df)\n",
    "\n",
    "players = pd.concat(dfs)\n",
    "players.to_csv(\"players.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Team Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    url = team_stats_url.format(year)\n",
    "    \n",
    "    data = requests.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    with open(\"team/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for year in years:\n",
    "    with open(\"team/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    \n",
    "    soup = bs(page, 'html.parser')\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    e_table = soup.find_all(id=\"divs_standings_E\")[0]\n",
    "    e_df = pd.read_html(str(e_table))[0]\n",
    "    e_df[\"Year\"] = year\n",
    "    e_df[\"Team\"] = e_df[\"Eastern Conference\"]\n",
    "    del e_df[\"Eastern Conference\"]\n",
    "    for i in range(len(e_df[\"Team\"])):\n",
    "        e_df[\"Team\"][i] = e_df[\"Team\"][i].split(\"*\")[0]\n",
    "\n",
    "    dfs.append(e_df)\n",
    "    \n",
    "    w_table = soup.find_all(id=\"divs_standings_W\")[0]\n",
    "    w_df = pd.read_html(str(w_table))[0]\n",
    "    w_df[\"Year\"] = year\n",
    "    w_df[\"Team\"] = w_df[\"Western Conference\"]\n",
    "    del w_df[\"Western Conference\"]\n",
    "    for i in range(len(w_df[\"Team\"])):\n",
    "        w_df[\"Team\"][i] = w_df[\"Team\"][i].split(\"*\")[0]\n",
    "    dfs.append(w_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.concat(dfs)\n",
    "teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams.to_csv(\"teams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Wikipedia:WikiProject_National_Basketball_Association/National_Basketball_Association_team_abbreviations\"\n",
    "\n",
    "data = requests.get(url)\n",
    "with open(\"abb/name.html\", \"w+\") as f:\n",
    "  f.write(data.text)\n",
    "data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"abb/name.html\") as f:\n",
    "  page = f.read()\n",
    "soup = bs(page, 'html.parser')\n",
    "table = soup.find_all(\"table\", class_=\"wikitable sortable\")[0]\n",
    "[sup.decompose() for sup in soup(\"sup\")]\n",
    "abrev_df = pd.read_html(str(table))[0]\n",
    "\n",
    "for i in range(len(abrev_df['Abbreviation/ Acronym'])):\n",
    "  abrev_df['Abbreviation/ Acronym'][i] = abrev_df['Abbreviation/ Acronym'][i].split(' ')[0]\n",
    "\n",
    "abrev_df.to_csv(\"abrev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
